{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "track1_train_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "IrAaZuufjZMd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Mount to Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "Cm2u8QRMjWyJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0f9acc86-60fb-4952-a982-8c091bb4d5ad"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qXoUiLahjbYY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "830a7a5b-aab1-4052-9244-f06d8ba4ec85"
      },
      "cell_type": "code",
      "source": [
        "cd drive/My\\ Drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rCIi7wdTjbSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f239e458-bd01-4a6a-bce5-5bb239f75164"
      },
      "cell_type": "code",
      "source": [
        "cd Colab\\ Notebooks"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MSdjHkFUjbQN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca24d6fb-6e63-4e4a-ae83-fc98cf3a8dfa"
      },
      "cell_type": "code",
      "source": [
        "cd sim_self_driving_car"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/sim_self_driving_car\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jD5zLjCnjjpW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "install dependecies"
      ]
    },
    {
      "metadata": {
        "id": "royncACsjbPP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "0117aa17-3299-4c7d-fb74-dae9415e390d"
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 483.0MB 52.8MB/s \n",
            "\u001b[31mfastai 1.0.50.post1 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mfastai 1.0.50.post1 has requirement torch>=1.0.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[?25h0.4.1\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w7-Un1IAl-US",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "import libraries"
      ]
    },
    {
      "metadata": {
        "id": "8__UzUCXl926",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from torch.utils import data\n",
        "\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch.utils import data\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# use skimage if you do not have cv2 installed\n",
        "# from skimage import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q490yOVVj17Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "hyper-parameters"
      ]
    },
    {
      "metadata": {
        "id": "G04fXeJxjbNE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # directory\n",
        "# parser.add_argument('--dataroot',     type=str,   default=\"../data/\", help='path to dataset')\n",
        "# parser.add_argument('--ckptroot',     type=str,   default=\"./\",       help='path to checkpoint')\n",
        "\n",
        "# # hyperparameters settings\n",
        "# parser.add_argument('--lr',           type=float, default=0.0001,     help='learning rate')\n",
        "# parser.add_argument('--weight_decay', type=float, default=1e-5,       help='weight decay (L2 penalty)')\n",
        "# parser.add_argument('--batch_size',   type=int,   default=32,         help='training batch size')\n",
        "# parser.add_argument('--num_workers',  type=int,   default=8,          help='number of workers in dataloader')\n",
        "# parser.add_argument('--test_size',    type=float, default=0.8,        help='train validation set split ratio')\n",
        "# parser.add_argument('--shuffle',      type=bool,  default=True,       help='whether shuffle data during training')\n",
        "\n",
        "# # training settings\n",
        "# parser.add_argument('--epochs',       type=int,   default=10,         help='number of epochs to train')\n",
        "# parser.add_argument('--start_epoch',  type=int,   default=0,          help='pre-trained epochs')\n",
        "# parser.add_argument('--resume',       type=bool,  default=False,      help='whether re-training from ckpt')\n",
        "\n",
        "\n",
        "# track 2\n",
        "\n",
        "dataroot = \"./data/\"\n",
        "ckptroot = \"./\"\n",
        "\n",
        "lr = 0.0001\n",
        "weight_decay = 1e-5\n",
        "batch_size = 32\n",
        "num_workers = 8\n",
        "test_size = 0.8\n",
        "shuffle = True\n",
        "\n",
        "epochs = 50\n",
        "start_epoch = 0\n",
        "resume = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jm9LyN2vlqnH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "helper functions"
      ]
    },
    {
      "metadata": {
        "id": "zwbFge6_ltLB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def toDevice(datas, device):\n",
        "    \"\"\"Enable cuda.\"\"\"\n",
        "    imgs, angles = datas\n",
        "    return imgs.float().to(device), angles.float().to(device)\n",
        "\n",
        "\n",
        "def augment(dataroot, imgName, angle):\n",
        "    \"\"\"Data augmentation.\"\"\"\n",
        "    name = dataroot + 'IMG/' + imgName.split('/')[-1]\n",
        "    current_image = cv2.imread(name)\n",
        "#     current_image = io.imread(name)\n",
        "\n",
        "    current_image = current_image[65:-25, :, :]\n",
        "    if np.random.rand() < 0.5:\n",
        "        current_image = cv2.flip(current_image, 1)\n",
        "#         current_image = np.flipud(current_image)\n",
        "        angle = angle * -1.0\n",
        "\n",
        "    return current_image, angle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tvA6gVD_kA2x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "load data"
      ]
    },
    {
      "metadata": {
        "id": "l3SM21hwjbJo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data(data_dir, test_size):\n",
        "    \"\"\"Load training data and train validation split\"\"\"\n",
        "    pass\n",
        "\n",
        "    # reads CSV file into a single dataframe variable\n",
        "    data_df = pd.read_csv(os.path.join(data_dir, 'driving_log.csv'),\n",
        "                          names=['center', 'left', 'right', 'steering', 'throttle', 'reverse', 'speed'])\n",
        "\n",
        "    # Divide the data into training set and validation set\n",
        "    train_len = int(test_size * data_df.shape[0])\n",
        "    valid_len = data_df.shape[0] - train_len\n",
        "    trainset, valset = data.random_split(\n",
        "        data_df.values.tolist(), lengths=[train_len, valid_len])\n",
        "\n",
        "    return trainset, valset\n",
        "\n",
        "trainset, valset = load_data(dataroot, test_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ypL4mjrBl1Jh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "custom dataset"
      ]
    },
    {
      "metadata": {
        "id": "96wgplsPl2fe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TripletDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, dataroot, samples, transform=None):\n",
        "        self.samples = samples\n",
        "        self.dataroot = dataroot\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_samples = self.samples[index]\n",
        "        steering_angle = float(batch_samples[3])\n",
        "\n",
        "        center_img, steering_angle_center = augment(self.dataroot, batch_samples[0], steering_angle)\n",
        "        left_img, steering_angle_left = augment(self.dataroot, batch_samples[1], steering_angle + 0.4)\n",
        "        right_img, steering_angle_right = augment(self.dataroot, batch_samples[2], steering_angle - 0.4)\n",
        "\n",
        "        center_img = self.transform(center_img)\n",
        "        left_img = self.transform(left_img)\n",
        "        right_img = self.transform(right_img)\n",
        "\n",
        "        return (center_img, steering_angle_center), (left_img, steering_angle_left), (right_img, steering_angle_right)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EZ-j7n-8kGmv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "prepare dataset using `DataLoader`"
      ]
    },
    {
      "metadata": {
        "id": "7tslG1aJjbHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06985914-b5b6-4f9f-f8f4-96d3f213632e"
      },
      "cell_type": "code",
      "source": [
        "print(\"==> Preparing dataset ...\")\n",
        "def data_loader(dataroot, trainset, valset, batch_size, shuffle, num_workers):\n",
        "    \"\"\"Self-Driving vehicles simulator dataset Loader.\n",
        "\n",
        "    Args:\n",
        "        trainset: training set\n",
        "        valset: validation set\n",
        "        batch_size: training set input batch size\n",
        "        shuffle: whether shuffle during training process\n",
        "        num_workers: number of workers in DataLoader\n",
        "\n",
        "    Returns:\n",
        "        trainloader (torch.utils.data.DataLoader): DataLoader for training set\n",
        "        testloader (torch.utils.data.DataLoader): DataLoader for validation set\n",
        "    \"\"\"\n",
        "    transformations = transforms.Compose(\n",
        "        [transforms.Lambda(lambda x: (x / 127.5) - 1.0)])\n",
        "\n",
        "    # Load training data and validation data\n",
        "    training_set = TripletDataset(dataroot, trainset, transformations)\n",
        "    trainloader = DataLoader(training_set,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=num_workers)\n",
        "\n",
        "    validation_set = TripletDataset(dataroot, valset, transformations)\n",
        "    valloader = DataLoader(validation_set,\n",
        "                           batch_size=batch_size,\n",
        "                           shuffle=shuffle,\n",
        "                           num_workers=num_workers)\n",
        "\n",
        "    return trainloader, valloader\n",
        "\n",
        "\n",
        "trainloader, validationloader = data_loader(dataroot,\n",
        "                                            trainset, valset,\n",
        "                                            batch_size,\n",
        "                                            shuffle,\n",
        "                                            num_workers)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing dataset ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZRaPrut5k0IJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "model architecture"
      ]
    },
    {
      "metadata": {
        "id": "xyVzFk-ijbD7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7127437d-eb8d-4cd4-fefc-9627f5d0c737"
      },
      "cell_type": "code",
      "source": [
        "class NetworkNvidia(nn.Module):\n",
        "    \"\"\"NVIDIA model used in the paper.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize NVIDIA model.\n",
        "\n",
        "        NVIDIA model used\n",
        "            Image normalization to avoid saturation and make gradients work better.\n",
        "            Convolution: 5x5, filter: 24, strides: 2x2, activation: ELU\n",
        "            Convolution: 5x5, filter: 36, strides: 2x2, activation: ELU\n",
        "            Convolution: 5x5, filter: 48, strides: 2x2, activation: ELU\n",
        "            Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n",
        "            Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n",
        "            Drop out (0.5)\n",
        "            Fully connected: neurons: 100, activation: ELU\n",
        "            Fully connected: neurons: 50, activation: ELU\n",
        "            Fully connected: neurons: 10, activation: ELU\n",
        "            Fully connected: neurons: 1 (output)\n",
        "\n",
        "        the convolution layers are meant to handle feature engineering\n",
        "        the fully connected layer for predicting the steering angle.\n",
        "        \"\"\"\n",
        "        super(NetworkNvidia, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 24, 5, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv2d(24, 36, 5, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv2d(36, 48, 5, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv2d(48, 64, 3),\n",
        "            nn.ELU(),\n",
        "            nn.Conv2d(64, 64, 3),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(in_features=64 * 2 * 33, out_features=100),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(in_features=100, out_features=50),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(in_features=50, out_features=10),\n",
        "            nn.Linear(in_features=10, out_features=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        input = input.view(input.size(0), 3, 70, 320)\n",
        "        output = self.conv_layers(input)\n",
        "        # print(output.shape)\n",
        "        output = output.view(output.size(0), -1)\n",
        "        output = self.linear_layers(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Define model\n",
        "print(\"==> Initialize model ...\")\n",
        "model = NetworkNvidia()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Initialize model ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nm27cWCrlB-L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "optimizer and criterion"
      ]
    },
    {
      "metadata": {
        "id": "ymCoZEcgjbA0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define optimizer and criterion\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=lr,\n",
        "                       weight_decay=weight_decay)\n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8fVMcfO6lPxj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "resume training or not"
      ]
    },
    {
      "metadata": {
        "id": "iyx_N1p1lKHv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if resume:\n",
        "    print(\"==> Loading checkpoint ...\")\n",
        "    checkpoint = torch.load(\"model.h5\",\n",
        "                            map_location=lambda storage, loc: storage)\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P7AeIyDRlbK0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "scheduler"
      ]
    },
    {
      "metadata": {
        "id": "ENk30y61lKDG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# learning rate scheduler\n",
        "scheduler = StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "\n",
        "# transfer to gpu\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oxIBZbk-lkqJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "training"
      ]
    },
    {
      "metadata": {
        "id": "XUxaorvdl6g9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Trainer(object):\n",
        "    \"\"\"Trainer.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 ckptroot,\n",
        "                 model,\n",
        "                 device,\n",
        "                 epochs,\n",
        "                 criterion,\n",
        "                 optimizer,\n",
        "                 scheduler,\n",
        "                 start_epoch,\n",
        "                 trainloader,\n",
        "                 validationloader):\n",
        "        \"\"\"Self-Driving car Trainer.\n",
        "\n",
        "        Args:\n",
        "            model:\n",
        "            device:\n",
        "            epochs:\n",
        "            criterion:\n",
        "            optimizer:\n",
        "            start_epoch:\n",
        "            trainloader:\n",
        "            validationloader:\n",
        "\n",
        "        \"\"\"\n",
        "        super(Trainer, self).__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.epochs = epochs\n",
        "        self.ckptroot = ckptroot\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.start_epoch = start_epoch\n",
        "        self.trainloader = trainloader\n",
        "        self.validationloader = validationloader\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Training process.\"\"\"\n",
        "        self.model.to(self.device)\n",
        "        for epoch in range(self.start_epoch, self.epochs + self.start_epoch):\n",
        "            self.scheduler.step()\n",
        "            \n",
        "            # Training\n",
        "            train_loss = 0.0\n",
        "            self.model.train()\n",
        "\n",
        "            for local_batch, (centers, lefts, rights) in enumerate(self.trainloader):\n",
        "                # Transfer to GPU\n",
        "                centers, lefts, rights = toDevice(centers, self.device), toDevice(\n",
        "                    lefts, self.device), toDevice(rights, self.device)\n",
        "\n",
        "                # Model computations\n",
        "                self.optimizer.zero_grad()\n",
        "                datas = [centers, lefts, rights]\n",
        "                for data in datas:\n",
        "                    imgs, angles = data\n",
        "                    # print(\"training image: \", imgs.shape)\n",
        "                    outputs = self.model(imgs)\n",
        "                    loss = self.criterion(outputs, angles.unsqueeze(1))\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                    train_loss += loss.data.item()\n",
        "\n",
        "                if local_batch % 100 == 0:\n",
        "\n",
        "                    print(\"Training Epoch: {} | Loss: {}\".format(epoch, train_loss / (local_batch + 1)))\n",
        "\n",
        "            # Validation\n",
        "            self.model.eval()\n",
        "            valid_loss = 0\n",
        "            with torch.set_grad_enabled(False):\n",
        "                for local_batch, (centers, lefts, rights) in enumerate(self.validationloader):\n",
        "                    # Transfer to GPU\n",
        "                    centers, lefts, rights = toDevice(centers, self.device), toDevice(\n",
        "                        lefts, self.device), toDevice(rights, self.device)\n",
        "\n",
        "                    # Model computations\n",
        "                    self.optimizer.zero_grad()\n",
        "                    datas = [centers, lefts, rights]\n",
        "                    for data in datas:\n",
        "                        imgs, angles = data\n",
        "                        outputs = self.model(imgs)\n",
        "                        loss = self.criterion(outputs, angles.unsqueeze(1))\n",
        "\n",
        "                        valid_loss += loss.data.item()\n",
        "\n",
        "                    if local_batch % 100 == 0:\n",
        "                        print(\"Validation Loss: {}\\n\".format(valid_loss / (local_batch + 1)))\n",
        "\n",
        "            # Save model\n",
        "            if epoch % 5 == 0 or epoch == self.epochs + self.start_epoch - 1:\n",
        "\n",
        "                state = {\n",
        "                    'epoch': epoch + 1,\n",
        "                    'state_dict': self.model.state_dict(),\n",
        "                    'optimizer': self.optimizer.state_dict(),\n",
        "                }\n",
        "\n",
        "                self.save_checkpoint(state)\n",
        "\n",
        "    def save_checkpoint(self, state):\n",
        "        \"\"\"Save checkpoint.\"\"\"\n",
        "        if not os.path.exists(self.ckptroot):\n",
        "            os.makedirs(self.ckptroot)\n",
        "\n",
        "        torch.save(state, self.ckptroot + 'model-{}.h5'.format(state['epoch']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IWHonYgBlKBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "38082355-49dc-4e3c-e9aa-6f8da22d5d40"
      },
      "cell_type": "code",
      "source": [
        "print(\"==> Start training ...\")\n",
        "trainer = Trainer(ckptroot,\n",
        "                  model,\n",
        "                  device,\n",
        "                  epochs,\n",
        "                  criterion,\n",
        "                  optimizer,\n",
        "                  scheduler,\n",
        "                  start_epoch,\n",
        "                  trainloader,\n",
        "                  validationloader)\n",
        "trainer.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Start training ...\n",
            "Training Epoch: 0 | Loss: 0.6782910078763962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z8_R9bP-lJ-l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}